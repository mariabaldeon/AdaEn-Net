{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import logging\n",
    "from keras import optimizers\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import CSVLogger,ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TerminateOnNaN  \n",
    "from keras import backend as K\n",
    "from .AdaBN_3D import get_3DAda, prediction\n",
    "from math import sqrt\n",
    "from numpy import linalg as LA\n",
    "from keras.utils import to_categorical\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from .ImageGenerator_3d import ImageDataGenerator\n",
    "\n",
    "class MOEAD3D(object):\n",
    "    def __init__(self,parameters, X_train, X_test, y_train, y_test):\n",
    "        self.n_hyper=9\n",
    "        self.pop_size=parameters[\"pop_size\"]\n",
    "        self.nei_size=parameters[\"nei_size\"]\n",
    "        self.max_iter=parameters[\"max_gen\"]\n",
    "        self.penalty=parameters[\"penalty\"]\n",
    "        self.batch_size=parameters[\"batch_size\"]\n",
    "        self.w_tloss=parameters[\"alpha\"]\n",
    "        self.w_eloss=parameters[\"beta\"]\n",
    "        self.n_epochs=parameters[\"epochs\"]\n",
    "        self.ref_per=0.8\n",
    "        self.X_train_r=X_train\n",
    "        self.X_val_r=X_test\n",
    "        self.y_train_r=y_train\n",
    "        self.y_val_r=y_test\n",
    "        \n",
    "        # =========================================================================================================\n",
    "        # Hyperparameter Tuning\n",
    "        # p= Spatial Dropout probability uniform [0-0.7]\n",
    "        # ker_sizei= kernel size for the ith convolutional layer in the Residual framework. i=1,2,3 size=[1,3,5]\n",
    "        # num_filters= number of the initial filters. The filters will be doubled in the downsampling and halved in the upsampling size=[4,8,16,32,64]\n",
    "        # act_func= activation function throughout the architecture ['relu', 'elu']\n",
    "        # blocks= Total number of blocks downsampling+upsampling [3, 5, 7, 9]\n",
    "        # merge= type of merging layer in long connections [1-Add(), 0-Concatenation()]\n",
    "        # alpha= learning rate [10^-6, 10^-2]\n",
    "        # n_epochs= number of epochs to train per model\n",
    "        # n_hyper= number of hyperparameters to be changes\n",
    "        # Genotype=[p,ker_size1,ker_size2,ker_size3, num_filters,act_func,alpha, blocks, merge]\n",
    "        # =========================================================================================================\n",
    "        self.alpha= [1e-03,9e-04, 8e-04,7e-04,6e-04, 5e-04, 4e-04, 3e-04, 2e-04, 1e-04,\n",
    "               9e-05, 8e-05, 7e-05, 6e-05, 5e-05, 4e-05, 3e-05, 2e-05, 1e-05]\n",
    "        self.num_filters=[32,16,8]\n",
    "        self.p=[0,0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "        self.ker_size1=[1,3,5,7]\n",
    "        self.ker_size2=[1,3,5,7]\n",
    "        self.ker_size3=[1,3,5,7]\n",
    "        self.act_func=['relu', 'elu']\n",
    "        self.blocks=[3, 5, 7, 9]\n",
    "        self.merge=[0, 1]\n",
    "        \n",
    "     #Generates a random genotype\n",
    "    def gen_genotype(self):\n",
    "        pi=self.p[np.random.randint(0,len(self.p))]\n",
    "        ker_size1i=self.ker_size1[np.random.randint(0,len(self.ker_size1))]\n",
    "        ker_size2i=self.ker_size2[np.random.randint(0,len(self.ker_size2))]\n",
    "        ker_size3i=self.ker_size3[np.random.randint(0,len(self.ker_size3))]\n",
    "        num_filtersi=self.num_filters[np.random.randint(0,len(self.num_filters))]\n",
    "        act_funci=self.act_func[np.random.randint(0,len(self.act_func))]\n",
    "        alphai=self.alpha[np.random.randint(0,len(self.alpha))]\n",
    "        blocksi=self.blocks[np.random.randint(0,len(self.blocks))]\n",
    "        mergei=self.merge[np.random.randint(0,len(self.merge))]\n",
    "        gene=[pi,ker_size1i,ker_size2i,ker_size3i,num_filtersi,act_funci, alphai, blocksi, mergei]\n",
    "        return gene\n",
    "\n",
    "            # Generates the names of the logging files\n",
    "    def log_name(self, generation):\n",
    "        location=\"SearchLogs3D\"\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        logger=[]\n",
    "        weights_name=[]\n",
    "        params=[]\n",
    "        for i in range(0,self.pop_size):\n",
    "            name=location+\"/\"+str(generation)+'_'+str(i)+'_training.log'\n",
    "            w_name=location+\"/\"+str(generation)+'_'+str(i)+'_weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "            p_name=location+\"/\"+str(generation)+'_'+str(i)+'_res+alpha.log'\n",
    "            logger.append(name)\n",
    "            weights_name.append(w_name)\n",
    "            params.append(p_name)\n",
    "        return logger, weights_name, params\n",
    "    \n",
    "        #Generates weight vectors for the whole population/pop_size (lambda in MOEA). \n",
    "        #As in the paper, the weight is element of i/H for H=pop_size and i from 0 to H. Formula of H is specific for 2 OF.\n",
    "    def generate_weight_vectors(self, pop_size):\n",
    "        H=self.pop_size-1\n",
    "\n",
    "        #weight_vectors=the weights generated for the objective functions\n",
    "        weight_vectors=[]\n",
    "        for i in range(self.pop_size): \n",
    "            w_z1=(i/H)\n",
    "            w_z2=1-w_z1\n",
    "            weight_vectors.append((w_z1,w_z2))\n",
    "        return weight_vectors\n",
    "    \n",
    "    #Generates the matrix neighbors of an individual based on the weights of the individual and the weights of the rest of the population\n",
    "    #Compute the distance of any two vectors and generate the neighborhood\n",
    "    #In the matrix, the row number determines the index of the individual in the weight_vectors and the numbers \n",
    "    #in the row the index of the neighboring lambdas in the weight_vectors\n",
    "    def compute_neighbors(self, weight_vectors, nei_size):\n",
    "\n",
    "        #neighbor=matrix with the index of the neighbors\n",
    "        neighbor=np.zeros((len(weight_vectors), self.nei_size))\n",
    "\n",
    "        # for each weight\n",
    "        for j in range(len(weight_vectors)):\n",
    "            #Select the weight of the individual-> tuple with the weight for each OF \n",
    "            weight=weight_vectors[j]\n",
    "            distance = pd.DataFrame(columns=['i','dist'])\n",
    "\n",
    "            # Compute the distance to other weights\n",
    "            for i in range(len(weight_vectors)): \n",
    "            #Compare the initial weight with all other weights \n",
    "                #if i!=j:\n",
    "                weight_2=weight_vectors[i]\n",
    "                dist = sqrt( (weight_2[0] - weight[0])**2 + (weight_2[1] - weight[1])**2 )\n",
    "                #Save the distance with all weights\n",
    "                distance=distance.append({'i':i,'dist':dist}, ignore_index=True)\n",
    "\n",
    "            #Sort the index by distance\n",
    "            distance=distance.sort_values(by=['dist'])\n",
    "            dist_m=distance.as_matrix()\n",
    "            #Add only the top neighbor size index\n",
    "            neighbor[j,:]=dist_m[:self.nei_size,0]\n",
    "\n",
    "        return neighbor\n",
    "\n",
    "    #Randonmly generates one parent from the individual's neighbor\n",
    "    def generate_parents(self, nei_size,neighbor,individual_index, OF):\n",
    "\n",
    "        j=individual_index\n",
    "        #Randomly generate one index of the parent from the neighbor of the individual\n",
    "        parents=np.random.choice(neighbor[j],1, replace=False)\n",
    "\n",
    "        #Generates another parent if it is the same as the individual\n",
    "        while parents==individual_index: \n",
    "            parents=np.random.choice(neighbor[j],1, replace=False)\n",
    "\n",
    "        #Save the information in dataframes\n",
    "        parent1=OF.loc[OF['Var']=='x_0_'+str(int(parents)),['p','ker_size1','ker_size2','ker_size3','n_filters',\n",
    "                                                            'act_func','alpha', 'blocks', 'merge']]\n",
    "        individual=OF.loc[OF['Var']=='x_0_'+str(j),['p','ker_size1','ker_size2','ker_size3','n_filters',\n",
    "                                                    'act_func','alpha', 'blocks', 'merge']]\n",
    "\n",
    "        return parent1, individual\n",
    "\n",
    "    #Generate the child's hyperparameters through recombination of the individual and the parents\n",
    "    def recombination(self, parent1, individual, p):\n",
    "        #child_hyper=child's hyperparameters\n",
    "        child_hyper=[]\n",
    "\n",
    "        #Recombination. With p probability we select either of the parent's genotype  \n",
    "        for k in range(self.n_hyper): \n",
    "            #Generate the probability\n",
    "            prob=np.random.uniform(0,1)\n",
    "            if prob<self.p: \n",
    "                child_hyper.append(parent1.iloc[0,k])\n",
    "            if prob>=self.p :\n",
    "                child_hyper.append(individual.iloc[0,k])\n",
    "\n",
    "        return child_hyper\n",
    "\n",
    "    # Computes the mutation probabilitaty for each gene. \n",
    "    # Value reduces with more generations\n",
    "    #gen is the number of generation\n",
    "    def mutation_prob(self, gen): \n",
    "        fi_0=min(20/self.n_hyper, 1)\n",
    "        p_n=max(fi_0*(1-(math.log(gen-1+1)/(math.log(self.max_iter)))), 1/self.n_hyper)\n",
    "        return p_n\n",
    "    \n",
    "    #Does Mutation. With p probability we do a mutation to each gene.  \n",
    "    def mutation(self, p, child_hyper):\n",
    "\n",
    "        #Generate a new gene that will replace certain values of the child_hyper\n",
    "        mutation_gene=self.gen_genotype()\n",
    "\n",
    "        # for each gene\n",
    "        for i in range(len(child_hyper)): \n",
    "            # compute mutation probability\n",
    "            prob=np.random.uniform(0,1)    \n",
    "\n",
    "            if prob<=self.p: \n",
    "                # Make sure you are really changing the gene\n",
    "                while child_hyper[i]==mutation_gene[i]: \n",
    "                    mutation_gene=self.gen_genotype()\n",
    "\n",
    "\n",
    "                child_hyper[i]=mutation_gene[i]\n",
    "        return child_hyper\n",
    "\n",
    "    #Calculates the BI cost function of the neighbor and the new child\n",
    "    def calculate_BI(self, OF_neighbor,Z_ref, max_OF, child_OF, nei_weights):\n",
    "        #Obtains the OF of the neighbor\n",
    "        OF_nei=np.matrix([[OF_neighbor.iloc[0,1]],[OF_neighbor.iloc[0,2]]])\n",
    "\n",
    "        #Retrieves the OF of the minimum point. Normalize only architechture size\n",
    "        Zm_ref=np.matrix([[(Z_ref[1]-Z_ref[1])/(max_OF[1]-Z_ref[1])],[(Z_ref[2]-Z_ref[2])/(max_OF[2]-Z_ref[2])]])\n",
    "\n",
    "        #Retrieves the OF of the new individual\n",
    "        OF_chi=np.matrix([[child_OF[1]],[child_OF[2]]])\n",
    "\n",
    "        #Compute the BI OF\n",
    "        d1_nei=LA.norm((OF_nei-Zm_ref).T*nei_weights)/LA.norm(nei_weights)\n",
    "        d2_nei=LA.norm(OF_nei-(Zm_ref+d1_nei*(nei_weights/LA.norm(nei_weights))))\n",
    "        ObjFunc_nei=d1_nei+self.penalty*d2_nei\n",
    "\n",
    "        #Compute the BI OF of the child\n",
    "        d1_chi=LA.norm((OF_chi-Zm_ref).T*nei_weights)/LA.norm(nei_weights)\n",
    "        d2_chi=LA.norm(OF_chi-(Zm_ref+d1_chi*(nei_weights/LA.norm(nei_weights))))\n",
    "        ObjFunc_chi=d1_chi+self.penalty*d2_chi\n",
    "\n",
    "        return ObjFunc_nei, ObjFunc_chi\n",
    "\n",
    "    #Computes the real pareto points from all the population checked\n",
    "    def real_pareto(self, models_checked):\n",
    "        models_checked1=models_checked.copy()\n",
    "        models_checked1=models_checked1.sort_values(['total_loss', 'param_count'])\n",
    "\n",
    "        #Create new components with the OF normalized (Adaptive Normalization)\n",
    "        models_checked1['train_loss_norm']=models_checked1['train_loss']\n",
    "        models_checked1['val_loss_norm']=models_checked1['val_loss']\n",
    "        models_checked1['total_loss_norm']=(models_checked1['total_loss']-Z_ref[1])/(max_OF[1]-Z_ref[1])\n",
    "        models_checked1['param_count_norm']=(models_checked1['param_count']-Z_ref[2])/(max_OF[2]-Z_ref[2])\n",
    "\n",
    "        s=0\n",
    "        OF3 =pd.DataFrame(columns=['Var','p','ker_size1','ker_size2','ker_size3','n_filters','act_func',\n",
    "                                   'alpha', 'blocks', 'merge',\n",
    "                                   'train_loss','val_loss','param_count','total_epochs', 'min_loss', 'total_loss',\n",
    "                                   'train_loss_norm', 'val_loss_norm', 'total_loss_norm',\n",
    "                                   'param_count_norm','real_var'])\n",
    "        OF3=OF3.append({'Var':'x_0_'+str(s),'p':models_checked1['p'].iloc[s],'ker_size1':models_checked1['ker_size1'].iloc[s],\n",
    "                        'ker_size2':models_checked1['ker_size2'].iloc[s],'ker_size3':models_checked1['ker_size3'].iloc[s],\n",
    "                        'n_filters':models_checked1['n_filters'].iloc[s],'act_func':models_checked1['act_func'].iloc[s],\n",
    "                        'alpha':models_checked1['alpha'].iloc[s],'blocks':models_checked1['blocks'].iloc[s], \n",
    "                        'merge':models_checked1['merge'].iloc[s], 'train_loss':models_checked1['train_loss'].iloc[s],\n",
    "                        'val_loss':models_checked1['val_loss'].iloc[s],'param_count':models_checked1['param_count'].iloc[s], \n",
    "                        'total_epochs': models_checked1['total_epochs'].iloc[s], \n",
    "                        'min_loss': models_checked1['min_loss'].iloc[s],\n",
    "                        'total_loss': models_checked1['total_loss'].iloc[s],\n",
    "                        'train_loss_norm':models_checked1['train_loss_norm'].iloc[s],\n",
    "                        'val_loss_norm':models_checked1['val_loss_norm'].iloc[s],\n",
    "                        'total_loss_norm':models_checked1['total_loss_norm'].iloc[s],\n",
    "                        'param_count_norm':models_checked1['param_count_norm'].iloc[s], \n",
    "                        'real_var':models_checked1['Var'].iloc[s]}, ignore_index=True)\n",
    "        last_param_count=OF3['param_count_norm'].iloc[0]\n",
    "        s=1\n",
    "        for i in range(1,len(models_checked1)): \n",
    "            last_param_count=OF3['param_count_norm'].iloc[s-1]\n",
    "            new_param_count=models_checked1['param_count_norm'].iloc[i]\n",
    "            if new_param_count<last_param_count: \n",
    "                OF3=OF3.append({'Var':'x_0_'+str(s),'p':models_checked1['p'].iloc[i],'ker_size1':models_checked1['ker_size1'].iloc[i],\n",
    "                                'ker_size2':models_checked1['ker_size2'].iloc[i],'ker_size3':models_checked1['ker_size3'].iloc[i],\n",
    "                                'n_filters':models_checked1['n_filters'].iloc[i],'act_func':models_checked1['act_func'].iloc[i],\n",
    "                                'alpha':models_checked1['alpha'].iloc[i],'blocks':models_checked1['blocks'].iloc[i],\n",
    "                                'merge':models_checked1['merge'].iloc[i], 'train_loss':models_checked1['train_loss'].iloc[i],\n",
    "                                'val_loss':models_checked1['val_loss'].iloc[i],'param_count':models_checked1['param_count'].iloc[i],\n",
    "                                'total_epochs': models_checked1['total_epochs'].iloc[i], \n",
    "                                'min_loss': models_checked1['min_loss'].iloc[i],\n",
    "                                'total_loss': models_checked1['total_loss'].iloc[s],\n",
    "                                'train_loss_norm':models_checked1['train_loss_norm'].iloc[i],\n",
    "                                'val_loss_norm':models_checked1['val_loss_norm'].iloc[i],\n",
    "                                'total_loss_norm':models_checked1['total_loss_norm'].iloc[s],\n",
    "                                'param_count_norm':models_checked1['param_count_norm'].iloc[i], \n",
    "                                'real_var':models_checked1['Var'].iloc[i]}, ignore_index=True)\n",
    "                s+=1\n",
    "        return OF3\n",
    "\n",
    "    #Total loss fc\n",
    "    def total_loss_fc(self, train_loss, val_loss, min_loss):\n",
    "        return self.w_tloss*train_loss+val_loss+self.w_eloss*((self.n_epochs-min_loss)/self.n_epochs)\n",
    "    \n",
    "    # Initialize the training model in keras\n",
    "    #loss coeficients\n",
    "    smooth=0.5\n",
    "    threshold=0\n",
    "    def dice_coef(self, y_true, y_pred):\n",
    "        y_true_f=K.flatten(y_true)\n",
    "        y_pred_f=K.flatten(y_pred)\n",
    "        intersection=K.sum(y_true_f*y_pred_f)\n",
    "        return(2.*intersection+smooth)/((K.sum(y_true_f*y_true_f)) + K.sum(y_pred_f*y_pred_f) + smooth)\n",
    "\n",
    "    def dice_coef_loss(y_true, y_pred):\n",
    "        return 1.-self.dice_coef(y_true, y_pred)\n",
    "\n",
    "    path='C:\\\\Users\\\\mariabaldeon\\\\Desktop\\\\Datasets\\\\Prostate MR Dataset\\\\3D\\\\V_Net\\\\128x128x32 (1mm,1mm,3mm)\\\\k4\\\\'\n",
    "    #path='C:\\\\Users\\\\mariabaldeon\\\\Documents\\\\Research\\\\3D AdaResU-Net\\\\Prostate\\\\Dataset\\\\VNet\\\\'\n",
    "\n",
    "    # Importing the pre processed data in the text file. \n",
    "    self.X_train_r= np.load(path+\"X4_trainVnt3mm.npy\")\n",
    "    self.X_val_r= np.load(path+\"X4_testVnt3mm.npy\")\n",
    "    self.y_train_r= np.load(path+\"y4_trainVnt3mm.npy\")\n",
    "    self.y_val_r= np.load(path+\"y4_testVnt3mm.npy\")\n",
    "\n",
    "    # Save information of the images\n",
    "    _, height, width, slices, channels=self.X_train_r.shape\n",
    "\n",
    "    # Resize the input matrix so that it satisfies (batch, x, y, z,channels)\n",
    "    print(self.X_train_r.shape)\n",
    "    print(self.y_train_r.shape)\n",
    "    print(self.X_val_r.shape)\n",
    "    print(self.y_val_r.shape)\n",
    "\n",
    "    # Normalize the data\n",
    "    self.X_train_r= pre_processing(self.X_train_r)\n",
    "    self.X_val_r= pre_processing(self.X_val_r)\n",
    "\n",
    "    print(np.max(self.X_train_r),np.min(self.X_train_r))\n",
    "    print(np.max(self.X_val_r),np.min(self.X_val_r))\n",
    "\n",
    "    print(np.unique(self.y_train_r))\n",
    "    print(np.unique(self.y_val_r))\n",
    "\n",
    "    #Data Generator for the X and Y, includes data augmentation\n",
    "    datagenX = ImageDataGenerator(rotation_range=90, width_shift_range=0.4, height_shift_range=0.4, zoom_range=0.5, horizontal_flip=True, data_format='channels_last')\n",
    "    datagenY = ImageDataGenerator(rotation_range=90, width_shift_range=0.4, height_shift_range=0.4, zoom_range=0.5, horizontal_flip=True, data_format='channels_last')\n",
    "\n",
    "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "    seed = 1\n",
    "\n",
    "    image_generator = datagenX.flow(self.X_train_r, self.batch_size=self.batch_size, seed=seed)\n",
    "    mask_generator = datagenY.flow(self.y_train_r, self.batch_size=self.batch_size, seed=seed)\n",
    "\n",
    "    # combine generators into one which yields image and masks\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "\n",
    "    # Genotype=[p,ker_size1,ker_size2,ker_size3, num_filters,act_func,alpha, blocks, merge]\n",
    "    #creates the model based on the gene(genotype) and trains based on backpropgation\n",
    "    #Saves all the information of the model in the logger files\n",
    "    #The gene must be a list with length equal to the number of hyperparameters to tune. \n",
    "    #It is expected that gene, logger, weights_name and params are for that particular individual\n",
    "    def model_train_bp(self, generation, gene, logger, weights_name, params, indv, height,width, slices, channels):\n",
    "        i=indv\n",
    "        model= get_3DAda(h=height,w=width, self.p=gene[0],k1=gene[1],k2=gene[2], k3=gene[3], nfilter=gene[4],actvfc=gene[5], \n",
    "                       self.blocks=gene[7], slices=slices, channels=channels, add=gene[8])\n",
    "        model.summary()\n",
    "        self.alpha=gene[6]\n",
    "\n",
    "        #Compile the model\n",
    "        adam=optimizers.Adam(lr=self.alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.compile(loss=self.dice_coef_loss, optimizer=adam)\n",
    "\n",
    "\n",
    "        #Stream epoch results to csv file \n",
    "        csv_logger = CSVLogger(logger)\n",
    "        model_check=ModelCheckpoint(filepath= weights_name , monitor='val_loss', verbose=0, save_best_only=True)\n",
    "        early_stopper=EarlyStopping(monitor='val_loss', min_delta=0.001, patience=20, mode='auto')\n",
    "        logging.basicConfig(filename=params, level=logging.INFO)\n",
    "        logging.info('generation: %s individual= %s p= %s k1= %s k2= %s k3= %s nfilter= %s act= %s alpha= %s blocks= %s add= %s', \n",
    "                     str(generation), str(i), str(gene[0]), str(gene[1]), str(gene[2]), str(gene[3]), str(gene[4]), \n",
    "                     str(gene[5]), str(gene[6]),str(gene[7]), str(gene[8]) )\n",
    "\n",
    "\n",
    "        #Fit the model\n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=(self.X_train_r.shape[0]/self.batch_size), \n",
    "                                    validation_data=(self.X_val_r, self.y_val_r), epochs=self.n_epochs, \n",
    "                                    callbacks=[csv_logger, model_check,early_stopper])\n",
    "\n",
    "\n",
    "        if math.isnan(history.history['loss'][-1]):\n",
    "            validation_loss=1000\n",
    "            train_loss=1000\n",
    "        elif math.isnan(history.history['val_loss'][-1]):\n",
    "            validation_loss=1000\n",
    "            train_loss=1000\n",
    "        else :\n",
    "            validation_loss=np.mean(history.history['val_loss'][-5:])\n",
    "            train_loss=np.mean(history.history['loss'][-5:])\n",
    "        train_parameters=model.count_params()\n",
    "        min_index=np.argmin(history.history['val_loss'])\n",
    "        total_epochs=len(history.history['val_loss'])\n",
    "\n",
    "\n",
    "        del model \n",
    "        K.clear_session()\n",
    "\n",
    "        return train_loss, validation_loss, train_parameters, total_epochs, min_index\n",
    "\n",
    "    # gene=[p,ker_size1,ker_size2,ker_size3, num_filters,act_func,alpha, blocks, merge]\n",
    "    start_time_algo = timeit.default_timer()\n",
    "    #1.1 Initialize the algorithm \n",
    "    #Randomly generate genotype for the initial population \n",
    "\n",
    "    #List that saves all the generated genotypes\n",
    "    genotype=[] \n",
    "\n",
    "    #Dataframe that saves all the information of each individual\n",
    "    models_checked = pd.DataFrame(columns=['Var','p','ker_size1','ker_size2','ker_size3','n_filters',\n",
    "                                           'act_func','alpha','blocks', 'merge', 'train_loss','val_loss','param_count',\n",
    "                                          'total_epochs', 'min_loss', 'total_loss'])\n",
    "\n",
    "    #Generates the logger names for the whole population in an specific generation\n",
    "    logger0, weights_name0, params0=self.log_name(0, self.pop_size)\n",
    "\n",
    "    #Creates the initial population\n",
    "    for i in range(0,self.pop_size): \n",
    "\n",
    "        gene=self.gen_genotype()\n",
    "        print(gene)\n",
    "        train_loss, validation_loss, train_params, total_epochs, min_loss =self.model_train_bp(0, gene, logger0[i], weights_name0[i], \n",
    "                                                                params0[i],i, height, width, slices, channels)\n",
    "        # Compute total loss\n",
    "        total_loss=self.total_loss_fc(train_loss, validation_loss, min_loss)\n",
    "        print(total_loss)\n",
    "        genotype.append(gene)\n",
    "        models_checked = models_checked.append({'Var':'x_0_'+str(i),'p':gene[0],'ker_size1':gene[1],\n",
    "                                                'ker_size2':gene[2],'ker_size3':gene[3],'n_filters':gene[4],\n",
    "                                                'act_func':gene[5],'alpha':gene[6], 'blocks':gene[7], \n",
    "                                                'merge':gene[8],'train_loss':train_loss,\n",
    "                                                'val_loss':validation_loss,'param_count':train_params,\n",
    "                                               'total_epochs':total_epochs,'min_loss':min_loss, \n",
    "                                               'total_loss': total_loss}, ignore_index=True)\n",
    "\n",
    "    #1.2. Calculate the reference point\n",
    "    model= get_3DAda(h=height,w=width,k1=int(np.min(self.ker_size1)),k2=int(np.min(self.ker_size2)), k3=int(np.min(self.ker_size3)),\n",
    "                   nfilter=int(np.min(self.num_filters)), self.blocks=int(np.min(self.blocks)), slices=slices, channels=channels )\n",
    "    min_parameters=model.count_params()\n",
    "\n",
    "    Z1_min=self.ref_per*np.min(models_checked['train_loss'])\n",
    "    Z2_min=self.ref_per*np.min(models_checked['total_loss']) \n",
    "    Z3_min=min_parameters\n",
    "\n",
    "    Z_ref=[Z1_min,Z2_min,Z3_min]    \n",
    "\n",
    "    #index for evolution of population\n",
    "    start=1\n",
    "\n",
    "    #1.3 Generate the uniformly distributed weight vectors\n",
    "\n",
    "    #Generate the weights of the objective functions for the whole population/pop_size weight\n",
    "    weight_vectors=self.generate_weight_vectors(self.pop_size)\n",
    "\n",
    "    #Determine the neighbors for each individual based on the distance of the weights\n",
    "    neighbor=self.compute_neighbors(weight_vectors, self.nei_size)\n",
    "\n",
    "    #STEP 2 Evolution!\n",
    "\n",
    "    #j = individual i=generation\n",
    "    OF=models_checked.copy()\n",
    "\n",
    "    #Initialize or update the vector of maximum value of OF for Adaptive Normalization\n",
    "    model=get_3DAda(h=height,w=width,k1=int(np.max(self.ker_size1)),k2=int(np.max(self.ker_size2)), k3=int(np.max(self.ker_size3)),\n",
    "                   nfilter=int(np.max(self.num_filters)),self.blocks=int(np.max(self.blocks)), slices=slices, channels=channels )\n",
    "    max_parameters=model.count_params()\n",
    "\n",
    "    # Dice coefficient of the validation+ Dice coefficient of the train*weight+ epoch with min loss \n",
    "    max_total_loss=2+1*self.w_tloss\n",
    "    max_OF=[1,max_total_loss,max_parameters]\n",
    "\n",
    "    #Do all for all generations\n",
    "    for i in range(start,self.max_iter): \n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        #Create new components with the OF normalized (Adaptive Normalization)\n",
    "        OF['train_loss_norm']=OF['train_loss']\n",
    "        OF['val_loss_norm']=OF['val_loss']\n",
    "        OF['total_loss_norm']=(OF['total_loss']-Z_ref[1])/(max_OF[1]-Z_ref[1])\n",
    "        OF['param_count_norm']=(OF['param_count']-Z_ref[2])/(max_OF[2]-Z_ref[2])\n",
    "\n",
    "        #Generates the logger names for the whole population in an specific generation\n",
    "        logger, weights_name, params=self.log_name(generation=i, self.pop_size=self.pop_size)\n",
    "\n",
    "        # Compute the probability of mutation. Since to compute the probability you assume we start in generation 2\n",
    "        prob=self.mutation_prob(i+1)\n",
    "\n",
    "        #Do for all individuals j in generation i\n",
    "        for j in range(self.pop_size):\n",
    "\n",
    "            #Randomly select one index from the neighborhood of j and generate a new candidate solution \"child_hyper\" \n",
    "            # from the parent and the individual \n",
    "            parent1, individual=self.generate_parents(self.nei_size,neighbor,j, OF)\n",
    "\n",
    "            #Generate the child's hyperparameters. With 1/2 prob we select either the parents genotype or the individuals genotype.  \n",
    "            child_hyper=self.recombination(parent1, individual, 1/2)\n",
    "\n",
    "            #Mutation.\n",
    "            child_hyper=self.mutation(prob, child_hyper)\n",
    "\n",
    "            #Assures the same models are not trained\n",
    "            while child_hyper in genotype:\n",
    "                #Mutation\n",
    "                child_hyper=self.mutation(prob, child_hyper)\n",
    "            print('child_hyper:', child_hyper)\n",
    "\n",
    "            #Train the child\n",
    "            train_loss, validation_loss, train_params, total_epochs, min_loss= self.model_train_bp(i, child_hyper, logger[j], \n",
    "                                                        weights_name[j], params[j],j, height, width, slices, channels)\n",
    "            # Compute total loss\n",
    "            total_loss=self.total_loss_fc(train_loss, validation_loss, min_loss)\n",
    "            print(total_loss)\n",
    "\n",
    "            models_checked = models_checked.append({'Var':'x_'+str(i)+'_'+str(j),'p':child_hyper[0],'ker_size1':child_hyper[1],\n",
    "                                                    'ker_size2':child_hyper[2],'ker_size3':child_hyper[3],\n",
    "                                                    'n_filters':child_hyper[4],'act_func':child_hyper[5],'alpha':child_hyper[6],\n",
    "                                                    'blocks':child_hyper[7], 'merge':child_hyper[8], 'train_loss':train_loss,\n",
    "                                                    'val_loss':validation_loss, 'param_count':train_params,\n",
    "                                                   'total_epochs':total_epochs,'min_loss':min_loss, \n",
    "                                                    'total_loss': total_loss}, ignore_index=True)\n",
    "            genotype.append(child_hyper)\n",
    "\n",
    "            #Adaptive Normalization\n",
    "            child_OF=[train_loss,(total_loss-Z_ref[1])/(max_OF[1]-Z_ref[1]), \n",
    "                      (train_params-Z_ref[2])/(max_OF[2]-Z_ref[2])]\n",
    "\n",
    "           #Calculate BI OF of each neighbor and compare with the child\n",
    "            for m in range(self.nei_size):\n",
    "                #m=neighbor\n",
    "                # select the index of the neighbor\n",
    "                neighbor_child=int(neighbor[j][m])\n",
    "                nei_weights=np.asarray(weight_vectors[neighbor_child]).reshape((2,1))\n",
    "\n",
    "                #Retrieves the OF values of the neighbor\n",
    "                OF_neighbor=OF.loc[OF['Var']=='x_0_'+str(neighbor_child),['train_loss_norm','total_loss_norm','param_count_norm']]\n",
    "\n",
    "                #Calculates the BI OF value for the neighbor m            \n",
    "                ObjFunc_nei, ObjFunc_chi=self.calculate_BI(OF_neighbor,Z_ref, max_OF, child_OF, nei_weights)\n",
    "\n",
    "                #If the maximum cost of the new child is less than the neighbor, replace as the new optimal OF\n",
    "                #If the cost of the new child is less than the neighbor, replace as the new optimal OF\n",
    "                if ObjFunc_chi<=ObjFunc_nei: \n",
    "                    #Eliminate the OF with lesser value than the child\n",
    "                    OF=OF[OF[\"Var\"]!='x_0_'+str(neighbor_child)]\n",
    "                    #Add the new pareto non optimal solution\n",
    "                    OF=OF.append({'Var':'x_0_'+str(neighbor_child),'p':child_hyper[0],'ker_size1':child_hyper[1],\n",
    "                                  'ker_size2':child_hyper[2],'ker_size3':child_hyper[3],'n_filters':child_hyper[4],\n",
    "                                  'act_func':child_hyper[5],'alpha':child_hyper[6],'blocks':child_hyper[7], \n",
    "                                  'merge':child_hyper[8], 'train_loss':train_loss, 'val_loss':validation_loss,\n",
    "                                  'param_count':train_params, 'total_epochs':total_epochs,'min_loss':min_loss,\n",
    "                                  'total_loss':total_loss,'train_loss_norm':child_OF[0], 'val_loss_norm':validation_loss,\n",
    "                                  'total_loss_norm':child_OF[1], 'param_count_norm':child_OF[2]}, ignore_index=True)\n",
    "                    OF=OF.sort_values(by=['Var'])\n",
    "\n",
    "            #Update the reference point\n",
    "            if self.ref_per*train_loss<Z_ref[0]: Z_ref[0]=self.ref_per*train_loss\n",
    "            if self.ref_per*total_loss<Z_ref[1]: Z_ref[1]=self.ref_per*total_loss\n",
    "\n",
    "            OF.to_csv('OF.csv')\n",
    "            models_checked.to_csv('models_checked.csv')\n",
    "            pareto_solutions=self.real_pareto(models_checked)\n",
    "            pareto_solutions.to_csv('pareto_solutions.csv')\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        logging.info('generation time: %s', str(elapsed))\n",
    "\n",
    "    #Save info into csv file\n",
    "    final_time=timeit.default_timer() -start_time_algo\n",
    "    OF.to_csv('OF.csv')\n",
    "    models_checked.to_csv('models_checked.csv')\n",
    "    logging.info('Time Elapsed: %s', str(final_time))\n",
    "    pareto_solutions=self.real_pareto(models_checked)\n",
    "    pareto_solutions.to_csv('pareto_solutions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
