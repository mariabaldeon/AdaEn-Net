{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import logging\n",
    "from keras import optimizers\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import CSVLogger,ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TerminateOnNaN  \n",
    "from keras import backend as K\n",
    "from .AdaBN_3D import get_3DAda, prediction\n",
    "from math import sqrt\n",
    "from numpy import linalg as LA\n",
    "from keras.utils import to_categorical\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from .ImageGenerator_3d import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOEAD3D(object):\n",
    "    def __init__(self,parameters, X_train, X_test, y_train, y_test):\n",
    "        self.n_hyper=9\n",
    "        self.pop_size=parameters[\"pop_size\"]\n",
    "        self.nei_size=parameters[\"nei_size\"]\n",
    "        self.max_iter=parameters[\"max_gen\"]\n",
    "        self.penalty=parameters[\"penalty\"]\n",
    "        self.batch_size=parameters[\"batch_size\"]\n",
    "        self.w_tloss=parameters[\"alpha\"]\n",
    "        self.w_eloss=parameters[\"beta\"]\n",
    "        self.n_epochs=parameters[\"epochs\"]\n",
    "        self.ref_per=0.8\n",
    "        self.X_train_r=X_train\n",
    "        self.X_val_r=X_test\n",
    "        self.y_train_r=y_train\n",
    "        self.y_val_r=y_test\n",
    "        \n",
    "        # =========================================================================================================\n",
    "        # Hyperparameter Tuning\n",
    "        # p= Spatial Dropout probability uniform [0-0.7]\n",
    "        # ker_sizei= kernel size for the ith convolutional layer in the Residual framework. i=1,2,3 size=[1,3,5]\n",
    "        # num_filters= number of the initial filters. The filters will be doubled in the downsampling and halved in the upsampling size=[4,8,16,32,64]\n",
    "        # act_func= activation function throughout the architecture ['relu', 'elu']\n",
    "        # blocks= Total number of blocks downsampling+upsampling [3, 5, 7, 9]\n",
    "        # merge= type of merging layer in long connections [1-Add(), 0-Concatenation()]\n",
    "        # alpha= learning rate [10^-6, 10^-2]\n",
    "        # n_epochs= number of epochs to train per model\n",
    "        # n_hyper= number of hyperparameters to be changes\n",
    "        # Genotype=[p,ker_size1,ker_size2,ker_size3, num_filters,act_func,alpha, blocks, merge]\n",
    "        # =========================================================================================================\n",
    "        self.alpha= [1e-03,9e-04, 8e-04,7e-04,6e-04, 5e-04, 4e-04, 3e-04, 2e-04, 1e-04,\n",
    "               9e-05, 8e-05, 7e-05, 6e-05, 5e-05, 4e-05, 3e-05, 2e-05, 1e-05]\n",
    "        self.num_filters=[32,16,8]\n",
    "        self.p=[0,0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "        self.ker_size1=[1,3,5,7]\n",
    "        self.ker_size2=[1,3,5,7]\n",
    "        self.ker_size3=[1,3,5,7]\n",
    "        self.act_func=['relu', 'elu']\n",
    "        self.blocks=[3, 5, 7, 9]\n",
    "        self.merge=[0, 1]\n",
    "        \n",
    "     #Generates a random genotype\n",
    "    def gen_genotype(self):\n",
    "        pi=self.p[np.random.randint(0,len(self.p))]\n",
    "        ker_size1i=self.ker_size1[np.random.randint(0,len(self.ker_size1))]\n",
    "        ker_size2i=self.ker_size2[np.random.randint(0,len(self.ker_size2))]\n",
    "        ker_size3i=self.ker_size3[np.random.randint(0,len(self.ker_size3))]\n",
    "        num_filtersi=self.num_filters[np.random.randint(0,len(self.num_filters))]\n",
    "        act_funci=self.act_func[np.random.randint(0,len(self.act_func))]\n",
    "        alphai=self.alpha[np.random.randint(0,len(self.alpha))]\n",
    "        blocksi=self.blocks[np.random.randint(0,len(self.blocks))]\n",
    "        mergei=self.merge[np.random.randint(0,len(self.merge))]\n",
    "        gene=[pi,ker_size1i,ker_size2i,ker_size3i,num_filtersi,act_funci, alphai, blocksi, mergei]\n",
    "        return gene\n",
    "\n",
    "            # Generates the names of the logging files\n",
    "    def log_name(self, generation):\n",
    "        location=\"SearchLogs3D\"\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        logger=[]\n",
    "        weights_name=[]\n",
    "        params=[]\n",
    "        for i in range(0,self.pop_size):\n",
    "            name=location+\"/\"+str(generation)+'_'+str(i)+'_training.log'\n",
    "            w_name=location+\"/\"+str(generation)+'_'+str(i)+'_weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "            p_name=location+\"/\"+str(generation)+'_'+str(i)+'_res+alpha.log'\n",
    "            logger.append(name)\n",
    "            weights_name.append(w_name)\n",
    "            params.append(p_name)\n",
    "        return logger, weights_name, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the names of the logging files\n",
    "def log_name(generation, pop_size):\n",
    "    logger=[]\n",
    "    weights_name=[]\n",
    "    params=[]\n",
    "    for i in range(0,pop_size):   \n",
    "        name=str(generation)+'_'+str(i)+'_training.log'\n",
    "        w_name=str(generation)+'_'+str(i)+'_weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "        p_name=str(generation)+'_'+str(i)+'_res+alpha.log'\n",
    "        logger.append(name)\n",
    "        weights_name.append(w_name)\n",
    "        params.append(p_name)\n",
    "    return logger, weights_name, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates weight vectors for the whole population/pop_size (lambda in MOEA). \n",
    "#As in the paper, the weight is element of i/H for H=pop_size and i from 0 to H. Formula of H is specific for 2 OF.\n",
    "def generate_weight_vectors(pop_size):\n",
    "    H=pop_size-1\n",
    "    \n",
    "    #weight_vectors=the weights generated for the objective functions\n",
    "    weight_vectors=[]\n",
    "    for i in range(pop_size): \n",
    "        w_z1=(i/H)\n",
    "        w_z2=1-w_z1\n",
    "        weight_vectors.append((w_z1,w_z2))\n",
    "    return weight_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates the matrix neighbors of an individual based on the weights of the individual and the weights of the rest of the population\n",
    "#Compute the distance of any two vectors and generate the neighborhood\n",
    "#In the matrix, the row number determines the index of the individual in the weight_vectors and the numbers \n",
    "#in the row the index of the neighboring lambdas in the weight_vectors\n",
    "def compute_neighbors(weight_vectors, nei_size):\n",
    "    \n",
    "    #neighbor=matrix with the index of the neighbors\n",
    "    neighbor=np.zeros((len(weight_vectors),nei_size))\n",
    "    \n",
    "    # for each weight\n",
    "    for j in range(len(weight_vectors)):\n",
    "        #Select the weight of the individual-> tuple with the weight for each OF \n",
    "        weight=weight_vectors[j]\n",
    "        distance = pd.DataFrame(columns=['i','dist'])\n",
    "        \n",
    "        # Compute the distance to other weights\n",
    "        for i in range(len(weight_vectors)): \n",
    "        #Compare the initial weight with all other weights \n",
    "            #if i!=j:\n",
    "            weight_2=weight_vectors[i]\n",
    "            dist = sqrt( (weight_2[0] - weight[0])**2 + (weight_2[1] - weight[1])**2 )\n",
    "            #Save the distance with all weights\n",
    "            distance=distance.append({'i':i,'dist':dist}, ignore_index=True)\n",
    "            \n",
    "        #Sort the index by distance\n",
    "        distance=distance.sort_values(by=['dist'])\n",
    "        dist_m=distance.as_matrix()\n",
    "        #Add only the top neighbor size index\n",
    "        neighbor[j,:]=dist_m[:nei_size,0]\n",
    "        \n",
    "    return neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randonmly generates one parent from the individual's neighbor\n",
    "def generate_parents(nei_size,neighbor,individual_index, OF):\n",
    "    \n",
    "    j=individual_index\n",
    "    #Randomly generate one index of the parent from the neighbor of the individual\n",
    "    parents=np.random.choice(neighbor[j],1, replace=False)\n",
    "    \n",
    "    #Generates another parent if it is the same as the individual\n",
    "    while parents==individual_index: \n",
    "        parents=np.random.choice(neighbor[j],1, replace=False)\n",
    "        \n",
    "    #Save the information in dataframes\n",
    "    parent1=OF.loc[OF['Var']=='x_0_'+str(int(parents)),['p','ker_size1','ker_size2','ker_size3','n_filters',\n",
    "                                                        'act_func','alpha', 'blocks', 'merge']]\n",
    "    individual=OF.loc[OF['Var']=='x_0_'+str(j),['p','ker_size1','ker_size2','ker_size3','n_filters',\n",
    "                                                'act_func','alpha', 'blocks', 'merge']]\n",
    "    \n",
    "    return parent1, individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the child's hyperparameters through recombination of the individual and the parents\n",
    "def recombination(parent1, individual, p):\n",
    "    #child_hyper=child's hyperparameters\n",
    "    child_hyper=[]\n",
    "    \n",
    "    #Recombination. With p probability we select either of the parent's genotype  \n",
    "    for k in range(n_hyper): \n",
    "        #Generate the probability\n",
    "        prob=np.random.uniform(0,1)\n",
    "        if prob<p: \n",
    "            child_hyper.append(parent1.iloc[0,k])\n",
    "        if prob>=p :\n",
    "            child_hyper.append(individual.iloc[0,k])\n",
    "            \n",
    "    return child_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the mutation probabilitaty for each gene. \n",
    "# Value reduces with more generations\n",
    "#gen is the number of generation\n",
    "def mutation_prob(gen): \n",
    "    fi_0=min(20/n_hyper, 1)\n",
    "    p_n=max(fi_0*(1-(math.log(gen-1+1)/(math.log(max_iter)))), 1/n_hyper)\n",
    "    return p_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does Mutation. With p probability we do a mutation to each gene.  \n",
    "def mutation(p, child_hyper):\n",
    "    \n",
    "    #Generate a new gene that will replace certain values of the child_hyper\n",
    "    mutation_gene=gen_genotype()\n",
    "    \n",
    "    # for each gene\n",
    "    for i in range(len(child_hyper)): \n",
    "        # compute mutation probability\n",
    "        prob=np.random.uniform(0,1)    \n",
    "\n",
    "        if prob<=p: \n",
    "            # Make sure you are really changing the gene\n",
    "            while child_hyper[i]==mutation_gene[i]: \n",
    "                mutation_gene=gen_genotype()\n",
    "\n",
    "            \n",
    "            child_hyper[i]=mutation_gene[i]\n",
    "    return child_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the BI cost function of the neighbor and the new child\n",
    "def calculate_BI(OF_neighbor,Z_ref, max_OF, child_OF, nei_weights):\n",
    "    #Obtains the OF of the neighbor\n",
    "    OF_nei=np.matrix([[OF_neighbor.iloc[0,1]],[OF_neighbor.iloc[0,2]]])\n",
    "       \n",
    "    #Retrieves the OF of the minimum point. Normalize only architechture size\n",
    "    Zm_ref=np.matrix([[(Z_ref[1]-Z_ref[1])/(max_OF[1]-Z_ref[1])],[(Z_ref[2]-Z_ref[2])/(max_OF[2]-Z_ref[2])]])\n",
    "\n",
    "    #Retrieves the OF of the new individual\n",
    "    OF_chi=np.matrix([[child_OF[1]],[child_OF[2]]])\n",
    "            \n",
    "    #Compute the BI OF\n",
    "    d1_nei=LA.norm((OF_nei-Zm_ref).T*nei_weights)/LA.norm(nei_weights)\n",
    "    d2_nei=LA.norm(OF_nei-(Zm_ref+d1_nei*(nei_weights/LA.norm(nei_weights))))\n",
    "    ObjFunc_nei=d1_nei+penalty*d2_nei\n",
    "\n",
    "    #Compute the BI OF of the child\n",
    "    d1_chi=LA.norm((OF_chi-Zm_ref).T*nei_weights)/LA.norm(nei_weights)\n",
    "    d2_chi=LA.norm(OF_chi-(Zm_ref+d1_chi*(nei_weights/LA.norm(nei_weights))))\n",
    "    ObjFunc_chi=d1_chi+penalty*d2_chi\n",
    "    \n",
    "    return ObjFunc_nei, ObjFunc_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the real pareto points from all the population checked\n",
    "def real_pareto(models_checked):\n",
    "    models_checked1=models_checked.copy()\n",
    "    models_checked1=models_checked1.sort_values(['total_loss', 'param_count'])\n",
    "\n",
    "    #Create new components with the OF normalized (Adaptive Normalization)\n",
    "    models_checked1['train_loss_norm']=models_checked1['train_loss']\n",
    "    models_checked1['val_loss_norm']=models_checked1['val_loss']\n",
    "    models_checked1['total_loss_norm']=(models_checked1['total_loss']-Z_ref[1])/(max_OF[1]-Z_ref[1])\n",
    "    models_checked1['param_count_norm']=(models_checked1['param_count']-Z_ref[2])/(max_OF[2]-Z_ref[2])\n",
    "\n",
    "    s=0\n",
    "    OF3 =pd.DataFrame(columns=['Var','p','ker_size1','ker_size2','ker_size3','n_filters','act_func',\n",
    "                               'alpha', 'blocks', 'merge',\n",
    "                               'train_loss','val_loss','param_count','total_epochs', 'min_loss', 'total_loss',\n",
    "                               'train_loss_norm', 'val_loss_norm', 'total_loss_norm',\n",
    "                               'param_count_norm','real_var'])\n",
    "    OF3=OF3.append({'Var':'x_0_'+str(s),'p':models_checked1['p'].iloc[s],'ker_size1':models_checked1['ker_size1'].iloc[s],\n",
    "                    'ker_size2':models_checked1['ker_size2'].iloc[s],'ker_size3':models_checked1['ker_size3'].iloc[s],\n",
    "                    'n_filters':models_checked1['n_filters'].iloc[s],'act_func':models_checked1['act_func'].iloc[s],\n",
    "                    'alpha':models_checked1['alpha'].iloc[s],'blocks':models_checked1['blocks'].iloc[s], \n",
    "                    'merge':models_checked1['merge'].iloc[s], 'train_loss':models_checked1['train_loss'].iloc[s],\n",
    "                    'val_loss':models_checked1['val_loss'].iloc[s],'param_count':models_checked1['param_count'].iloc[s], \n",
    "                    'total_epochs': models_checked1['total_epochs'].iloc[s], \n",
    "                    'min_loss': models_checked1['min_loss'].iloc[s],\n",
    "                    'total_loss': models_checked1['total_loss'].iloc[s],\n",
    "                    'train_loss_norm':models_checked1['train_loss_norm'].iloc[s],\n",
    "                    'val_loss_norm':models_checked1['val_loss_norm'].iloc[s],\n",
    "                    'total_loss_norm':models_checked1['total_loss_norm'].iloc[s],\n",
    "                    'param_count_norm':models_checked1['param_count_norm'].iloc[s], \n",
    "                    'real_var':models_checked1['Var'].iloc[s]}, ignore_index=True)\n",
    "    last_param_count=OF3['param_count_norm'].iloc[0]\n",
    "    s=1\n",
    "    for i in range(1,len(models_checked1)): \n",
    "        last_param_count=OF3['param_count_norm'].iloc[s-1]\n",
    "        new_param_count=models_checked1['param_count_norm'].iloc[i]\n",
    "        if new_param_count<last_param_count: \n",
    "            OF3=OF3.append({'Var':'x_0_'+str(s),'p':models_checked1['p'].iloc[i],'ker_size1':models_checked1['ker_size1'].iloc[i],\n",
    "                            'ker_size2':models_checked1['ker_size2'].iloc[i],'ker_size3':models_checked1['ker_size3'].iloc[i],\n",
    "                            'n_filters':models_checked1['n_filters'].iloc[i],'act_func':models_checked1['act_func'].iloc[i],\n",
    "                            'alpha':models_checked1['alpha'].iloc[i],'blocks':models_checked1['blocks'].iloc[i],\n",
    "                            'merge':models_checked1['merge'].iloc[i], 'train_loss':models_checked1['train_loss'].iloc[i],\n",
    "                            'val_loss':models_checked1['val_loss'].iloc[i],'param_count':models_checked1['param_count'].iloc[i],\n",
    "                            'total_epochs': models_checked1['total_epochs'].iloc[i], \n",
    "                            'min_loss': models_checked1['min_loss'].iloc[i],\n",
    "                            'total_loss': models_checked1['total_loss'].iloc[s],\n",
    "                            'train_loss_norm':models_checked1['train_loss_norm'].iloc[i],\n",
    "                            'val_loss_norm':models_checked1['val_loss_norm'].iloc[i],\n",
    "                            'total_loss_norm':models_checked1['total_loss_norm'].iloc[s],\n",
    "                            'param_count_norm':models_checked1['param_count_norm'].iloc[i], \n",
    "                            'real_var':models_checked1['Var'].iloc[i]}, ignore_index=True)\n",
    "            s+=1\n",
    "    return OF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss_fc(self, train_loss, val_loss, min_loss):\n",
    "    return self.w_tloss*train_loss+val_loss+self.w_eloss*((self.n_epochs-min_loss)/self.n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 128, 128, 32, 1)\n",
      "(40, 128, 128, 32, 1)\n",
      "(10, 128, 128, 32, 1)\n",
      "(10, 128, 128, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the training model in keras\n",
    "\n",
    "#loss coeficients\n",
    "smooth=0.5\n",
    "threshold=0\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f=K.flatten(y_true)\n",
    "    y_pred_f=K.flatten(y_pred)\n",
    "    intersection=K.sum(y_true_f*y_pred_f)\n",
    "    return(2.*intersection+smooth)/((K.sum(y_true_f*y_true_f)) + K.sum(y_pred_f*y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.-dice_coef(y_true, y_pred)\n",
    "\n",
    "path='C:\\\\Users\\\\mariabaldeon\\\\Desktop\\\\Datasets\\\\Prostate MR Dataset\\\\3D\\\\V_Net\\\\128x128x32 (1mm,1mm,3mm)\\\\k4\\\\'\n",
    "#path='C:\\\\Users\\\\mariabaldeon\\\\Documents\\\\Research\\\\3D AdaResU-Net\\\\Prostate\\\\Dataset\\\\VNet\\\\'\n",
    "\n",
    "# Importing the pre processed data in the text file. \n",
    "X_train_r= np.load(path+\"X4_trainVnt3mm.npy\")\n",
    "X_val_r= np.load(path+\"X4_testVnt3mm.npy\")\n",
    "y_train_r= np.load(path+\"y4_trainVnt3mm.npy\")\n",
    "y_val_r= np.load(path+\"y4_testVnt3mm.npy\")\n",
    "\n",
    "# Save information of the images\n",
    "_, height, width, slices, channels=X_train_r.shape\n",
    "\n",
    "# Resize the input matrix so that it satisfies (batch, x, y, z,channels)\n",
    "print(X_train_r.shape)\n",
    "print(y_train_r.shape)\n",
    "print(X_val_r.shape)\n",
    "print(y_val_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "1.0 0.0\n",
      "[0. 1.]\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "X_train_r= pre_processing(X_train_r)\n",
    "X_val_r= pre_processing(X_val_r)\n",
    "\n",
    "print(np.max(X_train_r),np.min(X_train_r))\n",
    "print(np.max(X_val_r),np.min(X_val_r))\n",
    "\n",
    "print(np.unique(y_train_r))\n",
    "print(np.unique(y_val_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 128, 128, 32, 1)\n",
      "(37, 128, 128, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "#Data Generator for the X and Y, includes data augmentation\n",
    "datagenX = ImageDataGenerator(rotation_range=90, width_shift_range=0.4, height_shift_range=0.4, zoom_range=0.5, horizontal_flip=True, data_format='channels_last')\n",
    "datagenY = ImageDataGenerator(rotation_range=90, width_shift_range=0.4, height_shift_range=0.4, zoom_range=0.5, horizontal_flip=True, data_format='channels_last')\n",
    "\n",
    "# Provide the same seed and keyword arguments to the fit and flow methods\n",
    "seed = 1\n",
    "\n",
    "image_generator = datagenX.flow(X_train_r, batch_size=batch_size, seed=seed)\n",
    "mask_generator = datagenY.flow(y_train_r, batch_size=batch_size, seed=seed)\n",
    "\n",
    "# combine generators into one which yields image and masks\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genotype=[p,ker_size1,ker_size2,ker_size3, num_filters,act_func,alpha, blocks, merge]\n",
    "#creates the model based on the gene(genotype) and trains based on backpropgation\n",
    "#Saves all the information of the model in the logger files\n",
    "#The gene must be a list with length equal to the number of hyperparameters to tune. \n",
    "#It is expected that gene, logger, weights_name and params are for that particular individual\n",
    "def model_train_bp(generation, gene, logger, weights_name, params, indv, height,width, slices, channels):\n",
    "    i=indv\n",
    "    model= get_3DAda(h=height,w=width, p=gene[0],k1=gene[1],k2=gene[2], k3=gene[3], nfilter=gene[4],actvfc=gene[5], \n",
    "                   blocks=gene[7], slices=slices, channels=channels, add=gene[8])\n",
    "    model.summary()\n",
    "    alpha=gene[6]\n",
    "\n",
    "    #Compile the model\n",
    "    adam=optimizers.Adam(lr=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss=dice_coef_loss, optimizer=adam)\n",
    " \n",
    "\n",
    "    #Stream epoch results to csv file \n",
    "    csv_logger = CSVLogger(logger)\n",
    "    model_check=ModelCheckpoint(filepath= weights_name , monitor='val_loss', verbose=0, save_best_only=True)\n",
    "    early_stopper=EarlyStopping(monitor='val_loss', min_delta=0.001, patience=20, mode='auto')\n",
    "    logging.basicConfig(filename=params, level=logging.INFO)\n",
    "    logging.info('generation: %s individual= %s p= %s k1= %s k2= %s k3= %s nfilter= %s act= %s alpha= %s blocks= %s add= %s', \n",
    "                 str(generation), str(i), str(gene[0]), str(gene[1]), str(gene[2]), str(gene[3]), str(gene[4]), \n",
    "                 str(gene[5]), str(gene[6]),str(gene[7]), str(gene[8]) )\n",
    "\n",
    "\n",
    "    #Fit the model\n",
    "    history=model.fit_generator(train_generator, steps_per_epoch=(X_train_r.shape[0]/batch_size), \n",
    "                                validation_data=(X_val_r, y_val_r), epochs=n_epochs, \n",
    "                                callbacks=[csv_logger, model_check,early_stopper])\n",
    "    \n",
    "    \n",
    "    if math.isnan(history.history['loss'][-1]):\n",
    "        validation_loss=1000\n",
    "        train_loss=1000\n",
    "    elif math.isnan(history.history['val_loss'][-1]):\n",
    "        validation_loss=1000\n",
    "        train_loss=1000\n",
    "    else :\n",
    "        validation_loss=np.mean(history.history['val_loss'][-5:])\n",
    "        train_loss=np.mean(history.history['loss'][-5:])\n",
    "    train_parameters=model.count_params()\n",
    "    min_index=np.argmin(history.history['val_loss'])\n",
    "    total_epochs=len(history.history['val_loss'])\n",
    "    \n",
    "    \n",
    "    del model \n",
    "    K.clear_session()\n",
    "    \n",
    "    return train_loss, validation_loss, train_parameters, total_epochs, min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene=[p,ker_size1,ker_size2,ker_size3, num_filters,act_func,alpha, blocks, merge]\n",
    "start_time_algo = timeit.default_timer()\n",
    "#1.1 Initialize the algorithm \n",
    "#Randomly generate genotype for the initial population \n",
    "\n",
    "#List that saves all the generated genotypes\n",
    "genotype=[] \n",
    "\n",
    "#Dataframe that saves all the information of each individual\n",
    "models_checked = pd.DataFrame(columns=['Var','p','ker_size1','ker_size2','ker_size3','n_filters',\n",
    "                                       'act_func','alpha','blocks', 'merge', 'train_loss','val_loss','param_count',\n",
    "                                      'total_epochs', 'min_loss', 'total_loss'])\n",
    "\n",
    "#Generates the logger names for the whole population in an specific generation\n",
    "logger0, weights_name0, params0=log_name(0, pop_size)\n",
    "\n",
    "#Creates the initial population\n",
    "for i in range(0,pop_size): \n",
    "    \n",
    "    gene=gen_genotype()\n",
    "    print(gene)\n",
    "    train_loss, validation_loss, train_params, total_epochs, min_loss =model_train_bp(0, gene, logger0[i], weights_name0[i], \n",
    "                                                            params0[i],i, height, width, slices, channels)\n",
    "    # Compute total loss\n",
    "    total_loss=total_loss_fc(train_loss, validation_loss, min_loss)\n",
    "    print(total_loss)\n",
    "    genotype.append(gene)\n",
    "    models_checked = models_checked.append({'Var':'x_0_'+str(i),'p':gene[0],'ker_size1':gene[1],\n",
    "                                            'ker_size2':gene[2],'ker_size3':gene[3],'n_filters':gene[4],\n",
    "                                            'act_func':gene[5],'alpha':gene[6], 'blocks':gene[7], \n",
    "                                            'merge':gene[8],'train_loss':train_loss,\n",
    "                                            'val_loss':validation_loss,'param_count':train_params,\n",
    "                                           'total_epochs':total_epochs,'min_loss':min_loss, \n",
    "                                           'total_loss': total_loss}, ignore_index=True)\n",
    "\n",
    "#1.2. Calculate the reference point\n",
    "model= get_3DAda(h=height,w=width,k1=int(np.min(ker_size1)),k2=int(np.min(ker_size2)), k3=int(np.min(ker_size3)),\n",
    "               nfilter=int(np.min(num_filters)), blocks=int(np.min(blocks)), slices=slices, channels=channels )\n",
    "min_parameters=model.count_params()\n",
    "\n",
    "Z1_min=ref_per*np.min(models_checked['train_loss'])\n",
    "Z2_min=ref_per*np.min(models_checked['total_loss']) \n",
    "Z3_min=min_parameters\n",
    "\n",
    "Z_ref=[Z1_min,Z2_min,Z3_min]    \n",
    "\n",
    "#index for evolution of population\n",
    "start=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 Generate the uniformly distributed weight vectors\n",
    "\n",
    "#Generate the weights of the objective functions for the whole population/pop_size weight\n",
    "weight_vectors=generate_weight_vectors(pop_size)\n",
    "\n",
    "#Determine the neighbors for each individual based on the distance of the weights\n",
    "neighbor=compute_neighbors(weight_vectors, nei_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF before first selection         Var     p  ker_size1  ker_size2  ker_size3  n_filters act_func  \\\n",
      "0     x_0_0  0.05          3          1          5         32      elu   \n",
      "1     x_0_1  0.15          5          5          1          4      elu   \n",
      "2     x_0_2  0.45          5          1          5         16     relu   \n",
      "3     x_0_3  0.15          5          3          1         16      elu   \n",
      "4     x_0_4  0.70          3          3          5         16      elu   \n",
      "5     x_0_5  0.50          3          5          5         32     relu   \n",
      "6     x_0_6  0.45          5          3          5          8     relu   \n",
      "7     x_0_7  0.10          5          3          5         32      elu   \n",
      "8     x_1_0  0.05          5          1          1          4      elu   \n",
      "9     x_1_1  0.05          5          5          1          4      elu   \n",
      "10    x_1_2  0.05          5          5          1          4      elu   \n",
      "11    x_1_3  0.05          3          1          1          8      elu   \n",
      "12    x_1_4  0.50          3          3          5          4      elu   \n",
      "13    x_1_5  0.50          3          1          5         32      elu   \n",
      "14    x_1_6  0.25          3          5          5         32      elu   \n",
      "15    x_1_7  0.55          3          1          5         32      elu   \n",
      "16    x_2_0  0.05          5          5          5          4      elu   \n",
      "17    x_2_1  0.30          3          5          5          8      elu   \n",
      "18    x_2_2  0.50          5          5          5          4      elu   \n",
      "19    x_2_3  0.50          5          3          5          4      elu   \n",
      "20    x_2_4  0.50          3          1          5         32     relu   \n",
      "21    x_2_5  0.50          3          1          5         32     relu   \n",
      "22    x_2_6  0.65          3          1          1         32      elu   \n",
      "23    x_2_7  0.50          1          1          5         32     relu   \n",
      "24    x_3_0  0.50          5          5          1          4      elu   \n",
      "25    x_3_1  0.00          3          3          3          8     relu   \n",
      "26    x_3_2  0.50          5          3          5          4     relu   \n",
      "27    x_3_3  0.50          5          5          5          4     relu   \n",
      "28    x_3_4  0.15          5          5          1         32      elu   \n",
      "29    x_3_5  0.50          5          1          1         32      elu   \n",
      "..      ...   ...        ...        ...        ...        ...      ...   \n",
      "289  x_36_2  0.50          3          1          5          4     relu   \n",
      "290  x_36_3  0.10          3          5          5         32     relu   \n",
      "291  x_36_4  0.50          3          1          1         32     relu   \n",
      "292  x_36_5  0.50          3          5          1         16     relu   \n",
      "293  x_36_6  0.60          3          5          1          4     relu   \n",
      "294  x_36_7  0.60          1          5          5         16      elu   \n",
      "295  x_37_0  0.15          3          1          1         32     relu   \n",
      "296  x_37_1  0.00          5          1          5         32     relu   \n",
      "297  x_37_2  0.45          3          5          5         32     relu   \n",
      "298  x_37_3  0.50          1          3          1         32     relu   \n",
      "299  x_37_4  0.00          3          1          1         32     relu   \n",
      "300  x_37_5  0.00          3          1          1         32     relu   \n",
      "301  x_37_6  0.15          1          5          1         32      elu   \n",
      "302  x_37_7  0.70          3          1          1         32      elu   \n",
      "303  x_38_0  0.60          3          5          5          8     relu   \n",
      "304  x_38_1  0.05          5          1          5         16     relu   \n",
      "305  x_38_2  0.55          1          1          3         32     relu   \n",
      "306  x_38_3  0.00          3          5          5         32     relu   \n",
      "307  x_38_4  0.60          3          3          1         16     relu   \n",
      "308  x_38_5  0.00          3          5          1         32     relu   \n",
      "309  x_38_6  0.15          3          5          1          4      elu   \n",
      "310  x_38_7  0.15          3          5          1         32      elu   \n",
      "311  x_39_0  0.70          3          1          5         32     relu   \n",
      "312  x_39_1  0.35          5          1          5          4     relu   \n",
      "313  x_39_2  0.30          1          3          1          8      elu   \n",
      "314  x_39_3  0.00          3          5          5         32     relu   \n",
      "315  x_39_4  0.10          1          1          5          4     relu   \n",
      "316  x_39_5  0.00          5          5          5         32     relu   \n",
      "317  x_39_6  0.00          3          3          3         16     relu   \n",
      "318  x_39_7  0.00          3          5          5         32      elu   \n",
      "\n",
      "            alpha  blocks  merge  train_loss  val_loss  param_count  \\\n",
      "0    2.000000e-05       9      0    0.183959  0.459538     65780641   \n",
      "1    4.000000e-03       5      1    0.327028  0.637243        84813   \n",
      "2    1.000000e-06       7      0    0.782855  0.775391      6200561   \n",
      "3    1.000000e-08       3      1    0.945379  0.930353       146097   \n",
      "4    1.000000e-03       5      1    0.274910  0.723614      1138769   \n",
      "5    6.000000e-07       7      1    0.794319  0.782012     29233569   \n",
      "6    6.000000e-06       7      1    0.663221  0.795181      1560377   \n",
      "7    4.000000e-08       9      1    0.841555  0.801040    100365281   \n",
      "8    2.000000e-05       9      0    0.724530  0.753407       718717   \n",
      "9    4.000000e-03       5      0    0.265350  0.498793        94813   \n",
      "10   4.000000e-03       5      1    0.229672  0.521324        84813   \n",
      "11   4.000000e-03       3      1    0.505962  0.746154         7689   \n",
      "12   8.000000e-04       5      0    0.408416  0.455165        73877   \n",
      "13   8.000000e-05       5      0    0.202571  0.275643      3995297   \n",
      "14   5.000000e-04       3      0    0.210501  0.478132      1665825   \n",
      "15   3.000000e-05       5      1    0.358329  0.403599      3857057   \n",
      "16   8.000000e-04       5      0    0.225576  0.555224       146397   \n",
      "17   7.000000e-03       5      0    0.209299  0.523871       457129   \n",
      "18   4.000000e-03       5      0    0.450715  0.641674       146397   \n",
      "19   4.000000e-03       5      0    0.466875  0.415314       105629   \n",
      "20   8.000000e-05       5      0    0.204173  0.246211      3995297   \n",
      "21   8.000000e-05       5      1    0.221737  0.318443      3857057   \n",
      "22   8.000000e-05       9      0    0.369983  0.581113     11688865   \n",
      "23   4.000000e-02       5      0    0.268232  0.582483      3461985   \n",
      "24   4.000000e-03       5      0    0.348002  0.619599        94813   \n",
      "25   4.000000e-02       3      0    0.518811  0.712429        29385   \n",
      "26   4.000000e-03       5      0    0.483803  0.603064       105629   \n",
      "27   4.000000e-07       9      0    0.836643  0.825033      2409085   \n",
      "28   1.000000e-04       5      0    0.145000  0.265458      6005473   \n",
      "29   8.000000e-05       5      0    0.196147  0.305228      2704097   \n",
      "..            ...     ...    ...         ...       ...          ...   \n",
      "289  5.000000e-02       7      0    0.721375  0.720420       256821   \n",
      "290  3.000000e-05       9      1    0.242594  0.484424    117522337   \n",
      "291  3.000000e-05       7      1    0.375879  0.728226      2314657   \n",
      "292  6.000000e-04       7      1    0.192332  0.437270      3946449   \n",
      "293  2.000000e-08       5      1    0.917398  0.887450        60901   \n",
      "294  8.000000e-05       3      0    0.433966  0.716070       390129   \n",
      "295  1.000000e-08       5      1    0.917458  0.876586       555681   \n",
      "296  3.000000e-05       3      0    0.146926  0.454472      1308513   \n",
      "297  2.000000e-06       9      1    0.511126  0.770170    117522337   \n",
      "298  3.000000e-05       9      0    0.581397  0.859498     13977697   \n",
      "299  8.000000e-05       7      0    0.102731  0.386267      2895265   \n",
      "300  8.000000e-05       7      1    0.129828  0.630761      2314657   \n",
      "301  3.000000e-04       5      1    0.120248  0.431385      3456865   \n",
      "302  8.000000e-05       5      0    0.436190  0.429755       693921   \n",
      "303  1.000000e-06       9      1    0.892458  0.862728      7349545   \n",
      "304  5.000000e-03       5      1    0.191460  0.593794      1343601   \n",
      "305  3.000000e-05       9      0    0.604388  0.834410     13977697   \n",
      "306  8.000000e-05       9      0    0.176334  0.511336    119872417   \n",
      "307  1.000000e-03       3      0    0.339529  0.431602        76177   \n",
      "308  8.000000e-05       5      0    0.167662  0.489160      3995297   \n",
      "309  8.000000e-06       5      1    0.812136  0.796785        60901   \n",
      "310  2.000000e-07       3      0    0.881995  0.807578       903969   \n",
      "311  3.000000e-05       9      1    0.548819  0.850405     63430561   \n",
      "312  5.000000e-05       5      0    0.850001  0.807275        94813   \n",
      "313  3.000000e-05       5      0    0.723545  0.571054        54233   \n",
      "314  3.000000e-05       9      0    0.237109  0.534407    119872417   \n",
      "315  1.000000e-05       9      0    0.794170  0.814954       888845   \n",
      "316  8.000000e-05       7      0    0.224367  0.510890     38246881   \n",
      "317  8.000000e-05       7      1    0.180628  0.522102      1992657   \n",
      "318  8.000000e-05       3      0    0.123254  0.403963      1665825   \n",
      "\n",
      "     total_epochs  min_loss Unnamed: 16  \n",
      "0              63        42         NaN  \n",
      "1              41        20         NaN  \n",
      "2              70        49         NaN  \n",
      "3              22         1         NaN  \n",
      "4             109        88         NaN  \n",
      "5              50        29         NaN  \n",
      "6              55        34         NaN  \n",
      "7             120       117         NaN  \n",
      "8              37        16         NaN  \n",
      "9              44        23         NaN  \n",
      "10             52        31         NaN  \n",
      "11             33        12         NaN  \n",
      "12             57        36         NaN  \n",
      "13             97        76         NaN  \n",
      "14             40        19         NaN  \n",
      "15             63        42         NaN  \n",
      "16             41        34         NaN  \n",
      "17             73        52         NaN  \n",
      "18             32        11         NaN  \n",
      "19             86        65         NaN  \n",
      "20             84        63         NaN  \n",
      "21             72        51         NaN  \n",
      "22             56        35         NaN  \n",
      "23             57        36         NaN  \n",
      "24             92        71         NaN  \n",
      "25             44        23         NaN  \n",
      "26             31        10         NaN  \n",
      "27            120       118         NaN  \n",
      "28             86        65         NaN  \n",
      "29            113        92         NaN  \n",
      "..            ...       ...         ...  \n",
      "289            28         7         NaN  \n",
      "290            44        23         NaN  \n",
      "291            56        46         NaN  \n",
      "292            66        45         NaN  \n",
      "293            24         3         NaN  \n",
      "294            83        62         NaN  \n",
      "295            29        13         NaN  \n",
      "296            52        36         NaN  \n",
      "297            75        54         NaN  \n",
      "298            26         5         NaN  \n",
      "299            50        29         NaN  \n",
      "300            56        35         NaN  \n",
      "301            88        67         NaN  \n",
      "302           120       111         NaN  \n",
      "303           120       107         NaN  \n",
      "304            32        11         NaN  \n",
      "305            39        18         NaN  \n",
      "306            44        23         NaN  \n",
      "307           105        86         NaN  \n",
      "308            39        18         NaN  \n",
      "309           120       109         NaN  \n",
      "310           120       118         NaN  \n",
      "311            23         2         NaN  \n",
      "312            44        23         NaN  \n",
      "313           120       119         NaN  \n",
      "314            45        24         NaN  \n",
      "315            49        48         NaN  \n",
      "316            30         9         NaN  \n",
      "317            35        14         NaN  \n",
      "318            73        52         NaN  \n",
      "\n",
      "[319 rows x 16 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-243-86e76a76889d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#Initialize or update the vector of maximum value of OF for Adaptive Normalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m model=get_3DAda(h=height,w=width,k1=int(np.max(ker_size1)),k2=int(np.max(ker_size2)), k3=int(np.max(ker_size3)),\n\u001b[1;32m----> 9\u001b[1;33m                nfilter=int(np.max(num_filters)),blocks=int(np.max(blocks)), slices=slices, channels=channels )\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mmax_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmax_OF\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_parameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\CLASS\\Research\\Research\\3D AdaResU-Net\\Prostate\\Ada Batch Norm\\MOEAD\\AdaBN_3D.py\u001b[0m in \u001b[0;36mget_3DAda\u001b[1;34m(h, w, p, k1, k2, k3, nfilter, actvfc, blocks, slices, channels, add)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mdown1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_downsampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_block\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfilter\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactvfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mdown2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_downsampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdown1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfilter\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactvfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mdown3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_downsampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdown2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfilter\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactvfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[0mdown4\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_downsampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdown3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfilter\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactvfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mup5\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_upsampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdown3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdown4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfilter\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactvfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\CLASS\\Research\\Research\\3D AdaResU-Net\\Prostate\\Ada Batch Norm\\MOEAD\\AdaBN_3D.py\u001b[0m in \u001b[0;36mres_downsampling\u001b[1;34m(previous_block, nfilter, k1, k2, k3, actvfc, p)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mx2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mConv3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'he_uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mx2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0mx2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactvfc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 431\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'moving_mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoving_mean_initializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             trainable=False)\n\u001b[0m\u001b[0;32m    125\u001b[0m         self.moving_variance = self.add_weight(\n\u001b[0;32m    126\u001b[0m             \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    250\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m                             constraint=constraint)\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weight_regularizer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mvariable\u001b[1;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\u001b[0m\n\u001b[0;32m    257\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m           \u001b[0mexpected_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\u001b[0m\n\u001b[0;32m    410\u001b[0m             self._try_guard_against_uninitialized_dependencies(\n\u001b[0;32m    411\u001b[0m                 self._initial_value),\n\u001b[1;32m--> 412\u001b[1;33m             validate_shape=validate_shape).op\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;31m# TODO(vrv): Change this class to not take caching_device, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m    214\u001b[0m     return gen_state_ops.assign(\n\u001b[0;32m    215\u001b[0m         \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    217\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m     61\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m     62\u001b[0m         \u001b[1;34m\"Assign\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m                 instructions)\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3154\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3155\u001b[0m           op_def=op_def)\n\u001b[1;32m-> 3156\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3157\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[1;34m(self, op, compute_device)\u001b[0m\n\u001b[0;32m   3249\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3250\u001b[0m       op._set_attr(\"_class\", attr_value_pb2.AttrValue(\n\u001b[1;32m-> 3251\u001b[1;33m           list=attr_value_pb2.AttrValue.ListValue(s=all_colocation_groups)))\n\u001b[0m\u001b[0;32m   3252\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_set_attr\u001b[1;34m(self, attr_name, attr_value)\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2175\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2176\u001b[1;33m       \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetAttr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2177\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2178\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#STEP 2 Evolution!\n",
    "\n",
    "#j = individual i=generation\n",
    "OF=models_checked.copy()\n",
    "\n",
    "#Initialize or update the vector of maximum value of OF for Adaptive Normalization\n",
    "model=get_3DAda(h=height,w=width,k1=int(np.max(ker_size1)),k2=int(np.max(ker_size2)), k3=int(np.max(ker_size3)),\n",
    "               nfilter=int(np.max(num_filters)),blocks=int(np.max(blocks)), slices=slices, channels=channels )\n",
    "max_parameters=model.count_params()\n",
    "\n",
    "# Dice coefficient of the validation+ Dice coefficient of the train*weight+ epoch with min loss \n",
    "max_total_loss=2+1*w_tloss\n",
    "max_OF=[1,max_total_loss,max_parameters]\n",
    "\n",
    "#Do all for all generations\n",
    "for i in range(start,max_iter): \n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    #Create new components with the OF normalized (Adaptive Normalization)\n",
    "    OF['train_loss_norm']=OF['train_loss']\n",
    "    OF['val_loss_norm']=OF['val_loss']\n",
    "    OF['total_loss_norm']=(OF['total_loss']-Z_ref[1])/(max_OF[1]-Z_ref[1])\n",
    "    OF['param_count_norm']=(OF['param_count']-Z_ref[2])/(max_OF[2]-Z_ref[2])\n",
    "    \n",
    "    #Generates the logger names for the whole population in an specific generation\n",
    "    logger, weights_name, params=log_name(generation=i, pop_size=pop_size)\n",
    "    \n",
    "    # Compute the probability of mutation. Since to compute the probability you assume we start in generation 2\n",
    "    prob=mutation_prob(i+1)\n",
    "    \n",
    "    #Do for all individuals j in generation i\n",
    "    for j in range(pop_size):\n",
    "        \n",
    "        #Randomly select one index from the neighborhood of j and generate a new candidate solution \"child_hyper\" \n",
    "        # from the parent and the individual \n",
    "        parent1, individual=generate_parents(nei_size,neighbor,j, OF)\n",
    "        \n",
    "        #Generate the child's hyperparameters. With 1/2 prob we select either the parents genotype or the individuals genotype.  \n",
    "        child_hyper=recombination(parent1, individual, 1/2)\n",
    "        \n",
    "        #Mutation.\n",
    "        child_hyper=mutation(prob, child_hyper)\n",
    "\n",
    "        #Assures the same models are not trained\n",
    "        while child_hyper in genotype:\n",
    "            #Mutation\n",
    "            child_hyper=mutation(prob, child_hyper)\n",
    "        print('child_hyper:', child_hyper)\n",
    "        \n",
    "        #Train the child\n",
    "        train_loss, validation_loss, train_params, total_epochs, min_loss= model_train_bp(i, child_hyper, logger[j], \n",
    "                                                    weights_name[j], params[j],j, height, width, slices, channels)\n",
    "        # Compute total loss\n",
    "        total_loss=total_loss_fc(train_loss, validation_loss, min_loss)\n",
    "        print(total_loss)\n",
    "        \n",
    "        models_checked = models_checked.append({'Var':'x_'+str(i)+'_'+str(j),'p':child_hyper[0],'ker_size1':child_hyper[1],\n",
    "                                                'ker_size2':child_hyper[2],'ker_size3':child_hyper[3],\n",
    "                                                'n_filters':child_hyper[4],'act_func':child_hyper[5],'alpha':child_hyper[6],\n",
    "                                                'blocks':child_hyper[7], 'merge':child_hyper[8], 'train_loss':train_loss,\n",
    "                                                'val_loss':validation_loss, 'param_count':train_params,\n",
    "                                               'total_epochs':total_epochs,'min_loss':min_loss, \n",
    "                                                'total_loss': total_loss}, ignore_index=True)\n",
    "        genotype.append(child_hyper)\n",
    "        \n",
    "        #Adaptive Normalization\n",
    "        child_OF=[train_loss,(total_loss-Z_ref[1])/(max_OF[1]-Z_ref[1]), \n",
    "                  (train_params-Z_ref[2])/(max_OF[2]-Z_ref[2])]\n",
    "        \n",
    "       #Calculate BI OF of each neighbor and compare with the child\n",
    "        for m in range(nei_size):\n",
    "            #m=neighbor\n",
    "            # select the index of the neighbor\n",
    "            neighbor_child=int(neighbor[j][m])\n",
    "            nei_weights=np.asarray(weight_vectors[neighbor_child]).reshape((2,1))\n",
    "            \n",
    "            #Retrieves the OF values of the neighbor\n",
    "            OF_neighbor=OF.loc[OF['Var']=='x_0_'+str(neighbor_child),['train_loss_norm','total_loss_norm','param_count_norm']]\n",
    "            \n",
    "            #Calculates the BI OF value for the neighbor m            \n",
    "            ObjFunc_nei, ObjFunc_chi=calculate_BI(OF_neighbor,Z_ref, max_OF, child_OF, nei_weights)\n",
    "                  \n",
    "            #If the maximum cost of the new child is less than the neighbor, replace as the new optimal OF\n",
    "            #If the cost of the new child is less than the neighbor, replace as the new optimal OF\n",
    "            if ObjFunc_chi<=ObjFunc_nei: \n",
    "                #Eliminate the OF with lesser value than the child\n",
    "                OF=OF[OF[\"Var\"]!='x_0_'+str(neighbor_child)]\n",
    "                #Add the new pareto non optimal solution\n",
    "                OF=OF.append({'Var':'x_0_'+str(neighbor_child),'p':child_hyper[0],'ker_size1':child_hyper[1],\n",
    "                              'ker_size2':child_hyper[2],'ker_size3':child_hyper[3],'n_filters':child_hyper[4],\n",
    "                              'act_func':child_hyper[5],'alpha':child_hyper[6],'blocks':child_hyper[7], \n",
    "                              'merge':child_hyper[8], 'train_loss':train_loss, 'val_loss':validation_loss,\n",
    "                              'param_count':train_params, 'total_epochs':total_epochs,'min_loss':min_loss,\n",
    "                              'total_loss':total_loss,'train_loss_norm':child_OF[0], 'val_loss_norm':validation_loss,\n",
    "                              'total_loss_norm':child_OF[1], 'param_count_norm':child_OF[2]}, ignore_index=True)\n",
    "                OF=OF.sort_values(by=['Var'])\n",
    "                  \n",
    "        #Update the reference point\n",
    "        if ref_per*train_loss<Z_ref[0]: Z_ref[0]=ref_per*train_loss\n",
    "        if ref_per*total_loss<Z_ref[1]: Z_ref[1]=ref_per*total_loss\n",
    "        \n",
    "        OF.to_csv('OF.csv')\n",
    "        models_checked.to_csv('models_checked.csv')\n",
    "        pareto_solutions=real_pareto(models_checked)\n",
    "        pareto_solutions.to_csv('pareto_solutions.csv')\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    logging.info('generation time: %s', str(elapsed))\n",
    "\n",
    "#Save info into csv file\n",
    "final_time=timeit.default_timer() -start_time_algo\n",
    "OF.to_csv('OF.csv')\n",
    "models_checked.to_csv('models_checked.csv')\n",
    "logging.info('Time Elapsed: %s', str(final_time))\n",
    "pareto_solutions=real_pareto(models_checked)\n",
    "pareto_solutions.to_csv('pareto_solutions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
